\section{Лекция 1}

\subsection{Основная задача математической статистики}
Для начала вспомним, чем вообще занимается теория вероятностей. Вообще, 
основной вопрос теории вероятности состоит в следующем: пусть есть случайная 
величина \(\xi\). Какова вероятность того, что \(\xi \in [a, b]\)? Чему равно 
её математическое ожидание и дисперсия?

Математическая статистика же занимается обратным вопросом: допустим, что нам 
известны некоторые наблюдения случайной величины \(\xi\). Как она себя ведёт?

Рассмотрим несколько задач математической статистики:

\begin{example}[Вопрос эмпирического выбора]
	Пусть в городе проживает \(N\) человек, среди которых \(M\) больно гриппом. 
	
	Примером задачи \textbf{теории вероятности} может служить следующий вопрос: 
	допустим, мы выбрали \(n\) человек. Какова вероятность того, что среди них 
	будет \(m\) заболевших? Стоит заметить, что для этой задачи \(M\) 
	предполагается известным.
	
	Задачей же \textbf{математической статистики} можно назвать следующую: 
	допустим, что на осмотр к врачу пришло \(n\) человек и оказалось, что из 
	них \(m\) больны. Сколько человек в городе болеет гриппом?
	
	Возможные ответы на данный вопрос:
    \begin{enumerate}
        \item $\lim\limits_{N \to \infty} \frac{m}{n}N = M$
        \item Можем указать интервал, в котором вероятно лежит искомая величина:\\ M $\in$ ($\tilde{M}$ - $\epsilon$, $\tilde{M}$ + $\epsilon$)
        \item Найти значение, которое будет меньше искомого: M > $M_{index}$
    \end{enumerate}
	
	Рассмотрим подробнее первый путь решения. Можно сказать, что выполнена следующая 
	сходимость: \(\frac{m}{n}N \prto M\) при \(n \to \infty\). Далее, можно 
	найти \(\epsilon > 0\) такой, что \(\Pr{|M - \frac{m}{n}N| >  \epsilon} 
	\leq 0.05\). В таком случае интервал \((\frac{m}{n}N - \epsilon,  
	\frac{m}{n}N + \epsilon)\) называется \emph{доверительным}.
\end{example}

\begin{example}[Регрессионная модель]
	Пусть была запущена ракета, и мы хотим оценить траекторию. При этом у нас 
	есть данные вида
	\[
		x_{i} = at_{i}^{2} + bt_{i} + \epsilon_{i},\quad i \in \{1, 2, \dots, 
		N\}
	\]
	где \(x_{i}\)~--- положение ракеты в момент времени \(t_{i}\), а 
	\(\epsilon_{i} \sim \mathcal{N}(0, \sigma^2)\)\footnote{Такое предположение 
	делается для упрощения вычислений}~--- ошибка вычислений. По этим данным 
	нужно оценить \(a\) и \(b\). Однако никто не обещал, что данные будут 
	однородны (т.е. распределение не обязано совпадать).
\end{example}

\begin{example}[Проверка однородности]
	Иногда мы хотим просто сказать нечто про распределение, а не искать его. 
	Допустим, что есть два набора \((X_{1}, \dots, X_{N})\) и \((Y_{1}, \dots, 
	Y_{N})\). Можно ли сказать, что для всех \(i\) выполнено, что \(X_{i} 
	\eqdist Y_{i}\)?
\end{example}

\begin{example}[Проверка независимости]
	Опять же, предположим, что у нас есть два набора \((X_{1}, \dots, X_{N})\) 
	и \((Y_{1}, \dots, Y_{N})\). Можно ли сказать, что для всех \(i\) 
	выполнено, что \(X_{i}\) независимо с \(Y_{i}\)? На практике примером такой 
	задачи служит связь цвета волос и цвета глаз.
\end{example}

\subsection{Обоснованность основной задачи  математической статистики}
Пусть Х -- наблюдение(случайный вектор), его распределение неизвестно. По Х хотим восстановить его распределение.

\label{label}
\begin{definition}
    Если Х = $(X_1, \ldots, X_n)$, где $X_i$ -- независимые одинаково распределённые случайные величины, то X называется $\emph{выборкой размера n}$
\end{definition}

В этом случае задача сводится к поиску распределения $X_1$.\\
\begin{definition}
$ P_{X_1}(B) = P(X_1 \in B)$, где B $\in$ $\mathfrak B$($\mathbb R$)
\end{definition}

\begin{definition}
    $\emph{Эмпирическим распределением выборки Х}$ называется 
\[
		P^{*}_{n}(B) = \frac{1}{n}\sum_{i = 1}^{n}\mathrm{I}\{X_{i} \in B\},
\]
\[ B \in \mathfrak B(\mathbb R)
\]
Это случайное распределение, являющееся дискретным равномерным распределением на $\{X_1, \ldots, X_n\}$
\end{definition}

\begin{proposition}
$$ P^{*}_{n}(B) \asto P(X_1 \in B)$$
\end{proposition}
\begin{proof}
По определению имеем:
$$P^{*}_{n}(B) = \frac{1}{n}\sum_{i = 1}^{n}\mathrm{I}\{X_{i} \in B\}$$
Так как $X_i$ -- независимые одинаково распределённые случайные величины, то cогласно УЗБЧ:\\
$$P^{*}_{n}(B) = \frac{1}{n}\sum_{i = 1}^{n}\mathrm{I}\{X_{i} \in B\} \asto \E\mathrm{I}\{X_{1} \in B\} = P(X_1 \in B) = P_{x_1}(B)$$
\end{proof}

Минус данного подхода: для конкретного множества работает хорошо, но одновременно всё распределение аппроксимировать нельзя. Можно показать, что 
$$ 
    \sup_{ B \in \mathfrak B(\mathbb R)}|P^{*}_{n}(B) - P_{X_1}(B)| = 1
$$

Пример, где такое достигается такой: \(P \sim \mathcal{N}(0,1)\) (или другое абсолютно непрерывное распределение), а \(B\) -- ровно множество выпавших точек \(X_1, \ldots, X_n\) (оно не детерминировано до выпадения, но существует, поэтому учтется при взятии супремума). Тогда получаем, что \(P^*_n(B) = 1, P_{X_1}(B) = 0\) и супремум соответственно равен \(1\).

\vspace{3pt}

Другой подход связан с похожим понятием:

\begin{definition}
    $\emph{Эмпирической функцией распределения выборки Х}$ называется
    $$
        F^{*}_{n}(x) = \frac{1}{n}\sum_{i = 1}^{n}\mathrm{I}\{X_{i} \leq x\}
    $$
\end{definition}

Его главным преимуществом являются две следующие теоремы:

\begin{theorem}[Гливенко--Кантелли]
Пусть F(x) -- настоящая функция распределения вектора $(X_1 \ldots X_n)$\\
Тогда
$$ \sup_{ x \in \mathbb R}|F^{*}_{n}(x) - F(x)| \asto 0
$$
\end{theorem}
вот сюда можно картиночку забубенькать

\begin{theorem}[Колмогорова]
Пусть F(x) -- настоящая функция распределения вектора $(X_1 \ldots X_n)$\\
Тогда
$$ \sqrt{n}\sup_{ x \in \mathbb R}|F^{*}_{n}(x) - F(x)| \dto \mathbb K
$$
Где $\mathbb K$ -- случайная величина с распределением Колмогорова
\end{theorem}